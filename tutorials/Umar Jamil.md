
# Umar Jamil -- a Machine Learning Coach

 -  Kolmogorov-Arnold Networks: MLP vs KAN, Math, B-Splines, Universal Approximation Theorem. \[[Video](https://youtu.be/-PFIkkwWdnM)\]\[[Slide](https://github.com/hkproj/kan-notes)\]
 -  Direct Preference Optimization (DPO) Explained: Bradley-Terry model, Log Probabilities, Math. \[[Video](https://youtu.be/hvGa5Mba4c8)\]\[[Slide](https://github.com/hkproj/dpo-notes)\]
 -  Reinforcement Learning from Human Feedback Explained with Math Derivations and the PyTorch Code. \[[Video](https://youtu.be/qGyFrqc34yc)\]\[[Slide](https://github.com/hkproj/rlhf-ppo)\]
 -  Mamba and S4 Explained: Architecture, Parallel Scan, Kernel Fusion, Recurrent, Convolution, Math. \[[Video](https://youtu.be/8Q_tqwpTpVU)\]\[[Slide](https://github.com/hkproj/mamba-notes)\]
 -  Mistral/Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer. \[[Video](https://youtu.be/UiX8K-xBUpE)\]\[[Slide](https://github.com/hkproj/mistral-llm-notes)\]
 -  Distributed Training with PyTorch: Complete Tutorial with Cloud Infrastructure and Code. \[[Video](https://youtu.be/toUSzwR0EV8)\]\[[Slide](https://github.com/hkproj/pytorch-transformer-distributed)\]
 -  Quantization Explained with PyTorch: Post-Training Quantization, Quantization-Aware Training. \[[Video](https://youtu.be/0VdNflU08yA)\]\[[Slide](https://github.com/hkproj/quantization-notes)\]
 -  Retrieval Augmented Generation (RAG) Explained: Embedding, Sentence BERT, Vector Database (HNSW). \[[Video](https://youtu.be/rhZgXNdhWDY)\]\[[Slide](https://github.com/hkproj/retrieval-augmented-generation-notes)\]
 -  BERT Explained: Training, Inference, BERT vs GPT/LLamA, Fine tuning, \[CLS\] token. \[[Video](https://youtu.be/90mGPxR2GgY)\]\[[Slide](https://github.com/hkproj/bert-from-scratch)\]
 -  Coding Stable Diffusion from Scratch in PyTorch. \[[Video](https://youtu.be/ZBKpAp_6TGI)\]\[[Slide](https://github.com/hkproj/pytorch-stable-diffusion)\]
 -  Coding LLaMA 2 from Scratch in PyTorch: KV Cache, Grouped Query Attention, Rotary PE, RMSNorm. \[[Video](https://youtu.be/oM4VmoabDAI)\]\[[Slide](https://github.com/hkproj/pytorch-llama)\]
 -  LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU. \[[Video](https://youtu.be/Mn_9W1nCFLo)\]\[[Slide](https://github.com/hkproj/pytorch-llama-notes)\]
 -  Segment Anything - Model Explanation with Code. \[[Video](https://youtu.be/Mn_9W1nCFLo)\]\[[Slide](https://github.com/hkproj/segment-anything-slides)\]
 -  LoRA: Low-Rank Adaptation of Large Language Models - Explained Visually + PyTorch Code from Scratch. \[[Video](https://youtu.be/PXWYUTMt-AU)\]\[[Slide](https://github.com/hkproj/pytorch-lora)\]
 -  LongNet: Scaling Transformers to 1,000,000,000 tokens - Python Code + Explanation. \[[Video](https://youtu.be/nC2nU9j9DVQ)\]\[[Slide](https://github.com/hkproj/python-longnet)\]
 -  How Diffusion Models Work - Explanation and Code! \[[Video](https://youtu.be/I1sPXkm2NH4)\]\[[Slide](https://github.com/hkproj/pytorch-ddpm)\]
 -  Variational Autoencoder - Model, ELBO, Loss Function and Maths explained easily! \[[Video](https://youtu.be/iwEzwTTalbg)\]\[[Slide](https://github.com/hkproj/vae-from-scratch-notes)\]
 -  Attention is all you need (Transformer) - Model explanation (including math), Inference and Training. \[[Video](https://youtu.be/bCz4OMemCcA)\]\[[Slide](https://github.com/hkproj/transformer-from-scratch-notes)\]
 -  Coding a Transformer from Scratch on PyTorch, with Full Explanation, Training and Inference. \[[Video](https://youtu.be/ISNdQcPhsts)\]\[[Slide](https://github.com/hkproj/pytorch-transformer)\]
 -  CLIP - Paper explanation (training and inference). \[[Video](https://youtu.be/L3BTG8ETY_Y)\]
 -  Wav2Lip (generate talking avatar videos) - Paper reading and explanation. \[[Video](https://youtu.be/n9ILOE2kyB0)\]

<!--
  vim:  ft=markdown ic noet norl wrap sw-8 ts=8 sts=4:
  -->
